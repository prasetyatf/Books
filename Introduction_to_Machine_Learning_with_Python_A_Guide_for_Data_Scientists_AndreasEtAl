Chapter 2: Supervised Learning
--Classification and Regression
y = Class/Categoric var -> binary classification / multiclass classification
y = Numeric
we want to build a model on the training data and then be able to make accurate predictions on new(able to generalize),-
unseen data that has the same characteristics as the training set that we used.
Overfitting = occurs when you fit a model too closely to the particularities of the training set and-
obtain a model that works well on the training set but is not able to generalize to new data.
Underfitting = Choosing too simple a model that not be able to capture all the aspects of and variability in the data,
and your model will do badly even on the training set.
--Relation of Model Complexity to Dataset Size
Never underestimate the power of more data.
the larger variety of data points your dataset contains, the more complex a model you can use without overfitting.
In the real world, you often have the ability to decide how much data to collect(but not duplicating same data points
or collecting very similiar data points), which might be more beneficial than tweaking and tuning your model. 
--Supervised ML Algorithm: Some sample Dataset
low-dimensional datasets = few features. vice versa
inspecting algorithms on low-dimensional datasets can be very instructive.
...
--Decision Tree
Essentially, they learn a hierarchy of if/else questions, leading to a decision.
the splitting process happen on tree while it devide data points into two part by a line on 2D graph.
a leaf on a tree means region on graph.
a metrices on tree means data points composition.
A leaf of the tree that contains data points that all share the same target value is called pure.
A prediction on a new data point is made by checking which region of the partition of the feature space-
the point lies in, and then predicting the majority target (or the single target in the case of pure leaves)-
in that region.(classification -> mode)
trees for regression tasks, To make a prediction, we traverse the tree based on the tests in each node and find
the leaf the new data point falls into. The output for this data point is the mean target of the training points
in this leaf.(regression -> mean)
--Controlling complexity of decision trees
building a tree means continuing until all leaves are pure leads to models that are very complex and-
highly overfit to the training data.
pure leaf means a tree 100% accurate on training set.
decision boundary focuses a lot on single outlier points that are far away from the other points in that class.
Common strategies to prevent overfitting:
pre-prunning = stopping the creation of the tree early.(can use sklearn)
post-pruning(pruning) = building the tree but then removing or collapsing nodes that contain little information.
(can't use sklearn).

Chapter 5: Model Evaluation & Improvement
...
Evaluation Metrics & Scoring
...it is important to choose the right metric when selecting between models and adjusting parameters.

--Keep The End Goal In Mind
not just making a prediction but using these predictions as part of a larger decisionmaking process
business metric = Before picking a machine learning metric, you should think about the high-level goal of the application.
business impact = The consequences of choosing a particular algorithm for a machine learning application.
In the early stages of development, and for adjusting parameters, it is often infeasible-
to put models into production just for testing purposes, because of the high business or personal risks that can be involved.

--Metrics for Binary Classification
--Kinds of Error
Often, accuracy is not a good measure of predictive performance, as the number of-
mistakes we make does not contain all the information we are interested in. 
For any application, we need to ask ourselves what the consequences of these mistakes might be in the real world.
false positive / type I error: Incorrect positive prediction -> a healthy guy detected has a cancer
false negative / type II error: Incorrect negative prediction -> a guy with cancer detected as healthy guy
--Imbalanced datasets
--Confusion matrices

...
Chapter 6: Algorithm Chains and Pipelines
