Chapter 2: Supervised Learning
--Classification and Regression
y = Class/Categoric var -> binary classification / multiclass classification
y = Numeric
we want to build a model on the training data and then be able to make accurate predictions on new(able to generalize),-
unseen data that has the same characteristics as the training set that we used.
Overfitting = occurs when you fit a model too closely to the particularities of the training set and-
obtain a model that works well on the training set but is not able to generalize to new data.
Underfitting = Choosing too simple a model that not be able to capture all the aspects of and variability in the data,
and your model will do badly even on the training set.
--Relation of Model Complexity to Dataset Size
Never underestimate the power of more data.
the larger variety of data points your dataset contains, the more complex a model you can use without overfitting.
In the real world, you often have the ability to decide how much data to collect(but not duplicating same data points
or collecting very similiar data points), which might be more beneficial than tweaking and tuning your model. 
--Supervised ML Algorithm: Some sample Dataset
low-dimensional datasets = few features. vice versa
inspecting algorithms on low-dimensional datasets can be very instructive.
...
--Decision Tree
Essentially, they learn a hierarchy of if/else questions, leading to a decision.
the splitting process happen on tree while it devide data points into two part by a line on 2D graph.
a leaf/node on a tree means region on graph.
a metrices on tree means data points composition.
A leaf/node of the tree that contains data points that all share the same target value is called pure.
A prediction on a new data point is made by checking which region of the partition of the feature space-
the point lies in, and then predicting the majority target (or the single target in the case of pure leaves)-
in that region.(classification -> mode)
trees for regression tasks, To make a prediction, we traverse the tree based on the tests in each node and find
the leaf the new data point falls into. The output for this data point is the mean target of the training points
in this leaf.(regression -> mean)
--Controlling complexity of decision trees
building a tree means continuing until all leaves are pure leads to models that are very complex and-
highly overfit to the training data.
pure leaf means a tree 100% accurate on training set.
decision boundary focuses a lot on single outlier points that are far away from the other points in that class.
Common strategies to prevent overfitting:
pre-prunning = stopping the creation of the tree early.(can use sklearn)
post-pruning(pruning) = building the tree but then removing or collapsing nodes that contain little information.
(can't use sklearn).
--Analyzing Decision Tree
for each nodes, there is:
feature = on root node, its indicate most importance feature(check using feature importance)
samples = its shows datapoint on the region
values = metrices that contain sum of each datapoints based on class
class = dominant datapoints
--Feautures Importance
the most importance feature should be the feature of root node.
f a feature has a low feature_importance, it doesn’t mean that this feature is uninformative.-
It only means that the feature was not picked by the tree, likely because another feature encodes the same information.
--forecast: DecisionTreeRegressor vs LinaeRegression
the linear model doing well on training data and test data, while tree model only good on training data.
--Strengths, Weakness, Parameters
pre-pruning: max_depth, max_leaf_nodes, or min_samples_leaf
adventages: visualized model easily understood, simple preprocessing(no need standarization, normalization),-
work well when you have features that are on completely different scales, or a mix of binary and con‐
tinuous features.
disadventages: even with the use of pre-pruning, they tend to overfit and provide poor generalization performance.
--Ensemble of Decision Tree
Ensembles = methods that combine multiple machine learning models to create more powerful models.
Ensemble Decision tree: random forests and gradient boosted decision trees.
...
--Uncertainty Estimates from Classifiers
In high-risk applications of machine learning, it is important to detect when the system is-
uncertain and can be mistaken. ex: false negative on cancer patient can lead to serious health problem.
sklearn: decision_function and predict_proba. most models have it, many have one of them.
gbrt.decision_function(X_test).shape --> the shape of decisison funtcion model(binary classification)
...

Chapter 5: Model Evaluation & Improvement
...
Cross validation
Cross Validation = scenario of splitting dataset(multiple splitting dataset)
benefit: evading lucky/unlucky effect, knowing how sensitive our model is to the selection of the training dataset-
(acuracy range), using more fold means we able tO use more portion of training set.
disadventage: computational cost.
Stratified k-fold
When data is ordered, its not wise using ordinary splitting. 
ex: ordered data = 0,0,1,1,2,2. train =0,0,1,1. test = 2,2
--More control over cros val
For most use cases, the defaults of k-fold crossvalidation for regression and stratified k-fold for classification 
work well, but there are some cases where you might want to use a different strategy.
if shuffle is true then fix the random_state
--Leave-one-out cross val
each fold is a single sample. For each split, you pick a single data point to be the test set.
not suitable for large dataset.
...
(there is many strategies actually)
Grid Search
is trying all possible combinations of the parameters of interest.
--Simple Grid Search
we tried combining parameters and select best acuracy.
--The Danger of Overfitting the Parameters and the Validation Set
we need an independent set for evaluating parameters
we have three sets: 
the training set to build the model, 
the validation (or development) set to select the parameters of the model, 
and the test set to evaluate the performance of the selected parameters.
--Grid Search With Cross Val
many people use the term cross-validation colloquially to refer to grid search with cross-validation.
common steps:
from sklearn.model_selection import GridSearchCV
specify the parameters you want to search over using a dictionary
set the parameters of gridsearch
split dataset to training set and test set
fit
score. btw, score is simple way to perform predict and acuracy_score.
best_params_ showing the best parameters. i suggest read more methodes, its very usefull.
Fitting the GridSearchCV object not only searches for the best parameters, but also
automatically fits a new model on the whole training dataset with the parameters that yielded the-
best cross-validation performance.
i think the gridsearchcv does automatically splitting dataset to validation set in the background.
-- Analyzing the result of cross-validation
i think its necessary only when we want to know the proccess behind it.
--Search over spaces that are not grids
be careful to tunning all possible parameters, bcs some parameters is conditional.
like parameters in SVM, if kernel='linear', then only parameter C is used.
To deal with these kinds of “conditional” parameters in SVM we can use:
param_grid = [{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},
 {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]
Evaluation Metrics & Scoring
...it is important to choose the right metric when selecting between models and adjusting parameters.

--Keep The End Goal In Mind
not just making a prediction but using these predictions as part of a larger decisionmaking process
business metric = Before picking a machine learning metric, you should think about the high-level goal of the application.
business impact = The consequences of choosing a particular algorithm for a machine learning application.
In the early stages of development, and for adjusting parameters, it is often infeasible-
to put models into production just for testing purposes, because of the high business or personal risks that can be involved.

--Metrics for Binary Classification
--Kinds of Error
Often, accuracy is not a good measure of predictive performance, as the number of-
mistakes we make does not contain all the information we are interested in. 
For any application, we need to ask ourselves what the consequences of these mistakes might be in the real world.
false positive / type I error: Incorrect positive prediction -> a healthy guy detected has a cancer
false negative / type II error: Incorrect negative prediction -> a guy with cancer detected as healthy guy
--Imbalanced datasets
one of two(many) classes is much more frequent than the other one.
this book is trying to predict imbalanced class with DummyClassifier and compare it using DecTree & LogReg,
DecTree & LogReg has moare accuracy than dummy, but only using accuracy is inadequate for imbalanced dataset
, we need metric to asses the model.
It is a classifier model that makes predictions without trying to find patterns in the data(source: towardatascience)
--Confusion matrices
its the most comprehensive ways to represent the result of evaluating binary classification.
the ouput: 2x2 metrices(array)
the rows = true classes
the columns = predicted classes
[[TN, FP],
[FN, TP]]
this book is trying to compare confusion matrix for each Most Freq Class, Dummy Model, DecTree, LogReg.
1.Most freq class  2.Dummy model  3.Decision Tree  4.Logistic Regression
[[403 0]           [[361 42]      [[390 13]        [[402 8]
[47 0]              [43 4]]       [24 23]]         [8 39]]
1. something is wrong with pred_most_frequent, because it always predicts the same class(negative class in this case).
2. pred_dummy, on the other hand, has a very small number of true positives (4), particularly compared to
the number of false negatives and false positives—there are many more false positives than true positives!
3. The predictions made by the decision tree make much more sense than the dummy predictions, 
even though the accuracy was nearly the same.
4. we can see that logistic regression does better than pred_tree in all aspects: it
has more true positives and true negatives while having fewer false positives and false negatives. 
my opiniion, look at the main diagonal, if its score better than the others, then its better.(True is correct prediction)
We can summerize confusion metric with things below:
--Relation to accuracy
-Accuracy
Accuracy = TP+TN / TP+TN+FP+FN
--Precision, recall, and f-score.
-Precision
Precision(positive predictive value (PPV)) = TP /TP+FP
Precision is used as a performance metric when the goal is to limit the number of false positives.
(In drug testing, Positive is that the drug is effective in treating the disease)
-Recall
Recall(sensitivity, hit rate, or true positive rate (TPR))= TP / TP+FN
(In cancer diagnosis, negative is that the people is healthy, so FN is wrong prediction about people are not sick)
Recall is used as performance metric when we need to identify all positive samples.
-There is a trade-off between optimizing recall and optimizing precision.
In the machine learning community, precision and recall are arguably the most commonly used
measures for binary classification, but other communities might use other related metrics.
-f-score or f-measure
F = 2 x ((precision x recall) / (precision + recall))
adventages: a better measure than accuracy on imbalanced binary classification datasets, we summarized the 
predictive performance again in one number
disadventages: it is harder to interpret and explain than accuracy

...
Chapter 6: Algorithm Chains and Pipelines
