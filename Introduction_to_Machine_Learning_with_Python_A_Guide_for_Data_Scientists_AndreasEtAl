Chapter 2: Supervised Learning
--Classification and Regression
y = Class/Categoric var -> binary classification / multiclass classification
y = Numeric
we want to build a model on the training data and then be able to make accurate predictions on new(able to generalize),-
unseen data that has the same characteristics as the training set that we used.
Overfitting = occurs when you fit a model too closely to the particularities of the training set and-
obtain a model that works well on the training set but is not able to generalize to new data.
Underfitting = Choosing too simple a model that not be able to capture all the aspects of and variability in the data,
and your model will do badly even on the training set.
--Relation of Model Complexity to Dataset Size
Never underestimate the power of more data.
the larger variety of data points your dataset contains, the more complex a model you can use without overfitting.
In the real world, you often have the ability to decide how much data to collect(but not duplicating same data points
or collecting very similiar data points), which might be more beneficial than tweaking and tuning your model. 
--Supervised ML Algorithm: Some sample Dataset
low-dimensional datasets = few features. vice versa
inspecting algorithms on low-dimensional datasets can be very instructive.
...
--Decision Tree
Essentially, they learn a hierarchy of if/else questions, leading to a decision.
the splitting process happen on tree while it devide data points into two part by a line on 2D graph.
a leaf/node on a tree means region on graph.
a metrices on tree means data points composition.
A leaf/node of the tree that contains data points that all share the same target value is called pure.
A prediction on a new data point is made by checking which region of the partition of the feature space-
the point lies in, and then predicting the majority target (or the single target in the case of pure leaves)-
in that region.(classification -> mode)
trees for regression tasks, To make a prediction, we traverse the tree based on the tests in each node and find
the leaf the new data point falls into. The output for this data point is the mean target of the training points
in this leaf.(regression -> mean)
--Controlling complexity of decision trees
building a tree means continuing until all leaves are pure leads to models that are very complex and-
highly overfit to the training data.
pure leaf means a tree 100% accurate on training set.
decision boundary focuses a lot on single outlier points that are far away from the other points in that class.
Common strategies to prevent overfitting:
pre-prunning = stopping the creation of the tree early.(can use sklearn)
post-pruning(pruning) = building the tree but then removing or collapsing nodes that contain little information.
(can't use sklearn).
--Analyzing Decision Tree
for each nodes, there is:
feature = on root node, its indicate most importance feature(check using feature importance)
samples = its shows datapoint on the region
values = metrices that contain sum of each datapoints based on class
class = dominant datapoints
--Feautures Importance
the most importance feature should be the feature of root node.
f a feature has a low feature_importance, it doesn’t mean that this feature is uninformative.-
It only means that the feature was not picked by the tree, likely because another feature encodes the same information.
--forecast: DecisionTreeRegressor vs LinaeRegression
the linear model doing well on training data and test data, while tree model only good on training data.
--Strengths, Weakness, Parameters
pre-pruning: max_depth, max_leaf_nodes, or min_samples_leaf
adventages: visualized model easily understood, simple preprocessing(no need standarization, normalization),-
work well when you have features that are on completely different scales, or a mix of binary and con‐
tinuous features.
disadventages: even with the use of pre-pruning, they tend to overfit and provide poor generalization performance.
--Ensemble of Decision Tree
Ensembles = methods that combine multiple machine learning models to create more powerful models.
Ensemble Decision tree: random forests and gradient boosted decision trees.
...
--Uncertainty Estimates from Classifiers


Chapter 5: Model Evaluation & Improvement
...
Evaluation Metrics & Scoring
...it is important to choose the right metric when selecting between models and adjusting parameters.

--Keep The End Goal In Mind
not just making a prediction but using these predictions as part of a larger decisionmaking process
business metric = Before picking a machine learning metric, you should think about the high-level goal of the application.
business impact = The consequences of choosing a particular algorithm for a machine learning application.
In the early stages of development, and for adjusting parameters, it is often infeasible-
to put models into production just for testing purposes, because of the high business or personal risks that can be involved.

--Metrics for Binary Classification
--Kinds of Error
Often, accuracy is not a good measure of predictive performance, as the number of-
mistakes we make does not contain all the information we are interested in. 
For any application, we need to ask ourselves what the consequences of these mistakes might be in the real world.
false positive / type I error: Incorrect positive prediction -> a healthy guy detected has a cancer
false negative / type II error: Incorrect negative prediction -> a guy with cancer detected as healthy guy
--Imbalanced datasets
--Confusion matrices

...
Chapter 6: Algorithm Chains and Pipelines
